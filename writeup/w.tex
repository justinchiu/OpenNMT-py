\documentclass{article}

\usepackage[margin=1in]{geometry}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}

% bib
\usepackage[round]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Latent Data to Document}

\begin{document}
\maketitle

\section{Introduction}

\section{Problem}
We would like to learn a generative model
over sentences $\bx = \{x_0, x_1, \ldots\}$ as well as distribution over latent tables $\bz$.  
We are primarily interested in the respective conditional distributions:
both the posterior distribution over tables given a sentence $p(\bz\mid\bx)$,
which is an information extraction model,
as well as the conditional distribution over summaries $p(\bx\mid\bx)$.

\section{Data to Document}
The conditional copy model in \citet{wiseman2017d2t}

\subsection{The Model}
Let $x_t$ be the current token, $\bx_{0:t-1}$ all previous tokens,
$z_t$ the prediction for the current score,
and $\tilde{\bz}_{0:t-1}\in\R^{t-1}_+$ be all previous scores.
The model takes the form a language model, where
the distribution over the next token
$$p(x_t\mid \bx_{0:t-1}, \tilde{\bz}_{0:t-1},z_t) = f([h_{l_t:t-1},h_t])$$

\section{Latent Table Model}
\subsection{Generative Model}
\begin{itemize}
\item $p(z)$ Is the prior over table completions,
which we will not focus on.
Or should we?
Can we set the prior to simply be the nearest neighbour plus some noise?
\item $p(x|z)$ is the likelihood of the summary given the table.
We use the conditional copy model as in \citet{wiseman2017d2t}.
\item $q(z|x)$ is the posterior distribution over table completions given a summary.
\end{itemize}

\subsection{Learning Conjunctions}
Introduce latent variable $\by$ and 

\subsection{Extending the supervision}
\begin{description}
\item[Content Selection] $p(\bc\mid\bx)$ where $\bc\in\set{0,1}^n$. 
\item[Content Ordering]
$p(\pi\mid\bc,\bx)$, where $\pi$ is a permutation matrix.
We may model this implicitly using a language model over relations,
i.e. debagging.
Error: we may have repeated records.
It may be possible that certain records are only referred to a single time
while we should allow others to be used multiple times.
\item[Relation Realization] $p(\by\mid\pi(\bc),\bx)$.
\end{description}

\section{Training and Inference}

\section{Related Work}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

